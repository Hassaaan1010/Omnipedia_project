# -*- coding: utf-8 -*-
"""Copy of nlp external prep.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15F1AsqHQuIN15O_WHBbC51Nf_0henX17
"""

pip install gensim==4.3.0

#exp2\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport spacy\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nnltk.download('punkt_tab') \nnlp = spacy.load("en_core_web_sm")\nparagraph = """\nA Natural Language Processing (NLP) pipeline.\n"""\ntokens = word_tokenize(paragraph)\nprint("Step 1: Tokenization\n", tokens)\nstop_words = set(stopwords.words('english'))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalnum()]\nprint("\nStep 2: Stop Word Removal\n", filtered_tokens)\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\nprint("\nStep 3: Stemming\n", stemmed_tokens)\nlemmatizer = WordNetLemmatizer()\nlemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\nprint("\nStep 4: Lemmatization (NLTK)\n", lemmatized_tokens)\ndoc = nlp(" ".join(filtered_tokens))\nspacy_lemmatized_tokens = [token.lemma_ for token in doc]\nprint("\nStep 4: Lemmatization (spaCy)\n", spacy_lemmatized_tokens)

#exp3\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\ndocuments = ["apple banana", "banana orange", "apple orange", "banana"]\nwords = set(" ".join(documents).split())\nword_to_index = {word: i for i, word in enumerate(words)}\none_hot_encoded = []\nfor word in words:\n    one_hot = [0] * len(words)\n    one_hot[word_to_index[word]] = 1\n    one_hot_encoded.append(one_hot)\nprint("One-Hot Encoding:\n", np.array(one_hot_encoded))\nvectorizer = CountVectorizer()\nbow_matrix = vectorizer.fit_transform(documents)\nprint("\nBag of Words:\n", bow_matrix.toarray())\nprint("Vocabulary:\n", vectorizer.vocabulary_)\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_matrix = tfidf_vectorizer.fit_transform(documents)\nprint("\nTF-IDF:\n", tfidf_matrix.toarray())\nprint("Vocabulary:\n", tfidf_vectorizer.vocabulary_)\nngram_vectorizer = CountVectorizer(ngram_range=(2, 2))  \nngram_matrix = ngram_vectorizer.fit_transform(documents)\nprint("\nN-Grams (Bigrams):\n", ngram_matrix.toarray())\nprint("Vocabulary:\n", ngram_vectorizer.vocabulary_)

#4\n#http://nlp.stanford.edu/data/glove.6B.zip for glove\n#alice.txt:- https://www.gutenberg.org/files/11/11-0.txt\nfrom gensim.models import Word2Vec\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nnltk.download('punkt')\nnltk.download('punkt_tab')\nfrom tensorflow.keras.preprocessing.text import Tokenizer \nwith open("/content/alice.txt", encoding='utf-8') as file:\n    text = file.read()\ntext = text.replace('\n', ' ')\nsentences = sent_tokenize(text)\ntokenized_sentences = [[word.lower() for word in word_tokenize(sent)] for sent in sentences]\ncbow_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=0)\nsg_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1)\nprint("CBOW similarity (alice vs wonderland):", cbow_model.wv.similarity('alice', 'wonderland'))\nprint("Skip-Gram similarity (alice vs wonderland):", sg_model.wv.similarity('alice', 'wonderland'))\nflat_text = [' '.join(sentence) for sentence in tokenized_sentences]\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(flat_text)\nword_index = tokenizer.word_index\ndef load_glove_embeddings(glove_path, word_index, embedding_dim=50):\n    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n    with open(glove_path, encoding='utf8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype='float32')\n            if .,word in word_index:\n                idx = word_index[word]\n                embedding_matrix[idx] = vector\n    return embedding_matrix\nglove_path = '/content/glove.6B.50d.txt'\nembedding_matrix = load_glove_embeddings(glove_path, word_index)\nprint("GloVe vector for 'alice':", embedding_matrix[word_index['alice']])

#exp5\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import movie_reviews, stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, classification_report, confusion_matrix\n)\nnltk.download('movie_reviews')\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\nstemmer = PorterStemmer()\ndef preprocess(doc):\n    words = [w.lower() for w in doc if w.isalpha()]  \n    words = [w for w in words if w not in stop_words]  \n    words = [stemmer.stem(w) for w in words]  \n    return ' '.join(words)\ndocs = [' '.join(movie_reviews.words(fileid)) for fileid in movie_reviews.fileids()]\nlabels = [1 if fileid.startswith('pos') else 0 for fileid in movie_reviews.fileids()]\ndocs = [preprocess(doc.split()) for doc in docs]  \nX_train, X_test, y_train, y_test = train_test_split(docs, labels, test_size=0.2, random_state=42)\nmodel = make_pipeline(CountVectorizer(), MultinomialNB())\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred) * 100\nprint(f"✅ Accuracy: {acc:.2f}%")\nprint("\n📊 Classification Report:")\nprint(classification_report(y_test, y_pred, target_names=["Negative", "Positive"]))\ncm = confusion_matrix(y_test, y_pred)\nprint("\n🧩 Confusion Matrix:")\nprint("            Predicted")\nprint("             0     1")\nprint(f"Actual 0    {cm[0][0]:<5} {cm[0][1]}")\nprint(f"       1    {cm[1][0]:<5} {cm[1][1]}")

#exp6 with data cleaning\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Conv1D, GlobalMaxPooling1D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\ndf = pd.read_csv('/content/spam.csv', encoding='latin-1')[['v1', 'v2']]\ndf.columns = ['label', 'text']\ndf['label'] = df['label'].map({'ham': 0, 'spam': 1})\ndf.dropna(inplace=True)\ntokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")\ntokenizer.fit_on_texts(df['text'])\nsequences = tokenizer.texts_to_sequences(df['text'])\npadded = pad_sequences(sequences, padding='post', maxlen=100)\nX = padded\ny = df['label'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndef build_model(model_type='lstm'):\n    model = Sequential()\n    model.add(Embedding(input_dim=5000, output_dim=64, input_length=100))\n    if model_type == 'lstm':\n        model.add(LSTM(64))\n    elif model_type == 'gru':\n        model.add(GRU(64))\n    elif model_type == 'cnn':\n        model.add(Conv1D(64, 5, activation='relu'))\n        model.add(GlobalMaxPooling1D())\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\nmodels = ['lstm', 'gru', 'cnn']\nfor model_type in models:\n    print(f"\n🏗️ Training {model_type.upper()} model...")\n    model = build_model(model_type)\n    history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), batch_size=32, verbose=0)\n    \n    y_pred = (model.predict(X_test) > 0.5).astype("int32")\n    print(f"📊 Results for {model_type.upper()}:\n")\n    print(classification_report(y_test, y_pred))

#exp7 with data cleaning\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, LayerNormalization, MultiHeadAttention, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\ndf = pd.read_csv('/content/spam.csv', encoding='latin-1')[['v1', 'v2']]\ndf.columns = ['label', 'text']\ndf['label'] = df['label'].map({'ham': 0, 'spam': 1})\ndf.dropna(inplace=True)\ntokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")\ntokenizer.fit_on_texts(df['text'])\nsequences = tokenizer.texts_to_sequences(df['text'])\npadded = pad_sequences(sequences, padding='post', maxlen=100)\nX = padded\ny = df['label'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ndef build_and_train_model(model_type, epochs=5):\n    inputs = Input(shape=(100,))\n    x = Embedding(input_dim=5000, output_dim=64)(inputs)\n    if model_type == 'self_attention':\n        attn_output = tf.keras.layers.Attention()([x, x])\n        x = GlobalAveragePooling1D()(attn_output)\n    elif model_type == 'multi_head_attention':\n        attn_output = MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\n        x = LayerNormalization()(x + attn_output)\n        x = GlobalAveragePooling1D()(x)\n        x = Dropout(0.1)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs, x)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    print(f"\n📘 Training {model_type.replace('_', ' ').title()} Model...\n")\n    model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_split=0.1)\n    \n    y_pred = (model.predict(X_test) > 0.5).astype("int32")\n    print(f"\n📊 Results for {model_type.replace('_', ' ').title()}:\n")\n    print(classification_report(y_test, y_pred))\nbuild_and_train_model('self_attention', epochs=5)\nbuild_and_train_model('multi_head_attention', epochs=5)

\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, Dense, GlobalAveragePooling1D, Dropout, LayerNormalization, MultiHeadAttention\nfrom sklearn.model_selection import train_test_split\ndf = pd.read_csv("IMDB Dataset.csv")\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\nvocab_size = 5000\nmaxlen = 100\ntokenizer = Tokenizer(num_words=vocab_size, oov_token="<OOV>")\ntokenizer.fit_on_texts(df['review'])\nsequences = tokenizer.texts_to_sequences(df['review'])\npadded_sequences = pad_sequences(sequences, maxlen=maxlen)\nx_train, x_test, y_train, y_test = train_test_split(\n    padded_sequences, df['sentiment'].values, test_size=0.2, random_state=42)\nprint("\n📘 Training Transformer-based Classifier...\n")\ninputs = Input(shape=(maxlen,))\nx = Embedding(vocab_size, 64)(inputs)\nattn_output = MultiHeadAttention(num_heads=2, key_dim=32)(x, x)\nx = LayerNormalization()(x + attn_output)\nx_ff = Dense(64, activation='relu')(x)\nx = LayerNormalization()(x + x_ff)\nx = GlobalAveragePooling1D()(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation='sigmoid')(x)\ntransformer_model = Model(inputs, x)\ntransformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\ntransformer_model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\nloss, accuracy = transformer_model.evaluate(x_test, y_test)\nprint(f"\n✅ Test Accuracy: {accuracy:.4f}")

#9\nimport pandas as pd\ndf = pd.read_csv('/content/legal_text_classification.csv')\nprint("Columns:", df.columns)\ndf['case_text'] = df['case_text'].astype(str)  \ndf = df.dropna(subset=['case_text'])  \ndf.head()\nimport nltk\nimport re\nimport heapq\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('punkt_tab')  \ndef extractive_summary(text, max_sentences=3):\n    \n    text = re.sub(r'\s+', ' ', text)\n    sentences = sent_tokenize(text)\n    stop_words = set(stopwords.words("english"))\n    \n    word_freq = {}\n    for word in word_tokenize(text.lower()):\n        if word.isalpha() and word not in stop_words:\n            word_freq[word] = word_freq.get(word, 0) + 1\n    \n    sentence_score = {}\n    for sent in sentences:\n        for word in word_tokenize(sent.lower()):\n            if word in word_freq:\n                sentence_score[sent] = sentence_score.get(sent, 0) + word_freq[word]\n    \n    summary_sentences = heapq.nlargest(max_sentences, sentence_score, key=sentence_score.get)\n    return ' '.join(summary_sentences)\nsample_text = df['case_text'].iloc[0]\nextractive_summary(sample_text)\nfrom transformers import T5ForConditionalGeneration, T5Tokenizer\nimport torch\nmodel = T5ForConditionalGeneration.from_pretrained("t5-small")\ntokenizer = T5Tokenizer.from_pretrained("t5-small")\ntext = """\nShehbaz is a dynamic and multifaceted student at Chaitanya Bharathi Institute of Technology (CBIT), Hyderabad, known across the campus for his exceptional leadership, creativity, and passion for community-building. As the Creative Vice Head of Neural Nexus, CBIT's premier technical club, Shehbaz has been instrumental in designing and leading a wide range of tech-focused events, workshops, and competitions that seamlessly merge innovation with creativity. His unique approach to organizing events is marked by attention to detail, an eye for design, and an infectious energy that inspires everyone around him. Whether it’s conceptualizing a themed hackathon or coordinating speaker sessions with industry veterans, Shehbaz brings structure, flair, and enthusiasm to everything he touches. His efforts have significantly elevated Neural Nexus’ visibility and impact on campus, earning him praise from both faculty and students alike.\nOutside of the tech sphere, Shehbaz is equally active in cultural and extracurricular domains. As part of the organizing committee for Shruthi 2025, CBIT’s flagship cultural fest, he played a key role in the creative planning, coordination, and execution of various large-scale events that brought together students from across Telangana and beyond. From managing backstage logistics to curating stage aesthetics and promotional material, Shehbaz has proven himself to be a reliable and resourceful force. He is also deeply invested in student welfare and often volunteers for college-wide initiatives aimed at improving campus life, such as mental health awareness campaigns, charity drives, and mentorship programs for juniors. His inclusive approach and ability to work with diverse teams make him a natural connector among students from different departments and backgrounds.\nBeyond academics and extracurriculars, Shehbaz is passionate about personal growth, often immersing himself in books on psychology, philosophy, and leadership. He’s a regular at college seminars and online courses, always seeking to expand his knowledge base and sharpen his skills. He maintains a balance between academics and activities with remarkable discipline, holding a respectable academic record while simultaneously being involved in multiple clubs and projects. Known among his peers as someone who never backs down from a challenge, Shehbaz consistently steps into roles that demand responsibility, creativity, and execution under pressure. With aspirations to become a leader in tech entrepreneurship, he is currently working on a project that aims to make augmented reality tools more accessible for educational use in under-resourced schools. Ambitious, thoughtful, and driven, Shehbaz represents the spirit of CBIT’s next-generation changemakers—grounded in values, guided by vision, and fueled by relentless action."""\npreprocessed_text = "summarize: " + text.strip().replace("\n", " ")\ninputs = tokenizer.encode(preprocessed_text, return_tensors="pt", max_length=512, truncation=True)\nsummary_ids = model.generate(inputs, max_length=100, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True)\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\nprint("\n--- Original Text ---")\nprint(text)\nprint("\n--- Abstractive Summary ---")\nprint(summary)

#exp10\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\ndef generate_text(prompt, model_name='gpt2', max_length=150, temperature=0.8, top_k=50):\n    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n    model = GPT2LMHeadModel.from_pretrained(model_name)\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    output = model.generate(\n        input_ids,\n        max_length=max_length,\n        temperature=temperature,\n        top_k=top_k,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n    return generated_text\nif __name__ == "__main__":\n    input_prompt = "Natural language processing is not"\n    result = generate_text(input_prompt)\n    print("\nInput prompt:")\n    print(input_prompt)\n    print("\nGenerated continuation:")\n    print(result)\n



pip uninstall jax jaxlib -y